{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Multipage Tables Scrape Demo\n",
    "\n",
    "You're often going to encounter data and tables that is spread across hundreds if not thousands of pages. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We're going to scrape as a demo a table that runs across several pages on this mock website.\n",
    "\n",
    "```https://sandeepmj.github.io/scrape-example-page/heaviest-animals-page1.html```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To capture your target information into a single CSV file will require the use of many of the foundational skills we've covered, including:\n",
    "\n",
    "- ```delays```\n",
    "- ```conditional logic```\n",
    "- ```while loops```\n",
    "- ```BeautifulSoup```\n",
    "\n",
    "\n",
    "And we'll explore a few new functional Python methods today."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Scraping Strategies\n",
    "\n",
    "- How do we approach this scrape?\n",
    "- What pattern do we see?\n",
    "- How do we capture a table on a single page?\n",
    "- How do we capture a sequence of tables?\n",
    "- How we navigate from page 1 to the subsequent pages?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Let's code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "from bs4 import BeautifulSoup  ## web scraping\n",
    "import requests ## request html for a page(s)\n",
    "import csv ## read or write to csv\n",
    "import pandas as pd ## pandas to work with data\n",
    "import re ## regular express in one of our functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We want to scrape:\n",
    "\n",
    "```https://sandeepmj.github.io/scrape-example-page/heaviest-animals-page1.html```\n",
    "\n",
    "But look at what I have assigned to the url variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## How is it different?\n",
    "url = \"https://sandeepmj.github.io/scrape-example-page/heaviest-animals-page{}.html\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Placeholders\n",
    "\n",
    "<img src=\"images/placeholder1.png\" style=\"width:500px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Placeholders\n",
    "\n",
    "<img src=\"images/placeholder2.png\" style=\"width:500px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Placeholders\n",
    "\n",
    "<img src=\"images/placeholder3.png\" style=\"width:500px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Filling the Placeholder\n",
    "\n",
    "### We use ```.format()``` to fill in values into the ```{}```placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://www.example{}.html'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## here's our base url\n",
    "base_link = \"http://www.example{}.html\"\n",
    "base_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "http://www.example0.html\n",
      "1\n",
      "http://www.example1.html\n",
      "2\n",
      "http://www.example2.html\n",
      "3\n",
      "http://www.example3.html\n",
      "4\n",
      "http://www.example4.html\n",
      "5\n",
      "http://www.example5.html\n",
      "6\n",
      "http://www.example6.html\n"
     ]
    }
   ],
   "source": [
    "## Using a ```for loop```\n",
    "all_urls_fl = []\n",
    "for url_number in range(0,7):\n",
    "    print(url_number)\n",
    "    print(base_link.format(url_number))\n",
    "    all_urls_fl.append(base_link.format(url_number))\n",
    "    \n",
    "# all_urls_fl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://www.example1.html',\n",
       " 'http://www.example2.html',\n",
       " 'http://www.example3.html',\n",
       " 'http://www.example4.html',\n",
       " 'http://www.example5.html',\n",
       " 'http://www.example6.html']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## using list comprehension\n",
    "all_urls_lc = [base_link.format(url_number) for url_number in range(1,7)]\n",
    "all_urls_lc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using f-strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "burl = \"http://www.example\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://www.example1.html',\n",
       " 'http://www.example2.html',\n",
       " 'http://www.example3.html',\n",
       " 'http://www.example4.html',\n",
       " 'http://www.example5.html',\n",
       " 'http://www.example6.html']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Using a ```for loop```\n",
    "all_urls_fs = []\n",
    "for url_number in range(1,7):\n",
    "    temp_url = f\"{burl}{url_number}.html\"\n",
    "    all_urls_fs.append(temp_url)\n",
    "    \n",
    "all_urls_fs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Back to our scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://sandeepmj.github.io/scrape-example-page/heaviest-animals-page{}.html'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## let's remind ourselves of url variable's value\n",
    "\n",
    "url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## We know we need a placeholder value of upto ```4```\n",
    "## Let's create a variable called  ```total_pages``` to match number of pages on site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## total pages to scrape\n",
    "total_pages = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://sandeepmj.github.io/scrape-example-page/heaviest-animals-page0.html\n",
      "404\n",
      "https://sandeepmj.github.io/scrape-example-page/heaviest-animals-page1.html\n",
      "200\n",
      "https://sandeepmj.github.io/scrape-example-page/heaviest-animals-page2.html\n",
      "200\n",
      "https://sandeepmj.github.io/scrape-example-page/heaviest-animals-page3.html\n",
      "200\n",
      "https://sandeepmj.github.io/scrape-example-page/heaviest-animals-page4.html\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "## Let's write the for loop\n",
    "## but instead of storing into a list, we just feed it directly to our placeholder\n",
    "## we want to just scape each page\n",
    "for url_number in range(0,total_pages):\n",
    "    link = url.format(url_number)\n",
    "    print(link)\n",
    "    site = requests.get(link)\n",
    "    print(site.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# HUGE PROBLEM\n",
    "\n",
    "### We're hitting the server way too fast. We have to add a delay before we proceed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## Let's import the required libaries to create a delay\n",
    "from random import randrange ##  allows us to randomize numbers library\n",
    "import time ## time tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://sandeepmj.github.io/scrape-example-page/heaviest-animals-page0.html\n",
      "snoozing for 15 seconds before scraping next link.\n",
      "https://sandeepmj.github.io/scrape-example-page/heaviest-animals-page1.html\n",
      "snoozing for 22 seconds before scraping next link.\n",
      "https://sandeepmj.github.io/scrape-example-page/heaviest-animals-page2.html\n",
      "snoozing for 16 seconds before scraping next link.\n",
      "https://sandeepmj.github.io/scrape-example-page/heaviest-animals-page3.html\n",
      "snoozing for 11 seconds before scraping next link.\n",
      "https://sandeepmj.github.io/scrape-example-page/heaviest-animals-page4.html\n",
      "snoozing for 18 seconds before scraping next link.\n"
     ]
    }
   ],
   "source": [
    "## Let's run our code again but with appropriate delay\n",
    "\n",
    "for url_number in range(0,total_pages):\n",
    "    link = url.format(url_number)\n",
    "    print(link)\n",
    "    snooze = randrange(10,25)\n",
    "    print(f\"snoozing for {snooze} seconds before scraping next link.\")\n",
    "    time.sleep(snooze)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Working Around Errors\n",
    "\n",
    "When you scrape hundreds of pages, there's chance that one of the URLs might be a dud.\n",
    "\n",
    "We can set up a error control to see what kind of responses we get:\n",
    "\n",
    "```<Response [200]>``` means website is accessible.\n",
    "\n",
    "```<Response [404]>``` means broken link or no page on content.\n",
    "\n",
    "In that case, your whole code might break and you'll have to figure out where it broke.\n",
    "\n",
    "We can make that easier with conditional logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oh no! https://sandeepmj.github.io/scrape-example-page/heaviest-animals-page0.html returned: 404\n",
      "got it...scraping page...https://sandeepmj.github.io/scrape-example-page/heaviest-animals-page1.html\n",
      "snoozing for 18 seconds before scraping next link.\n",
      "got it...scraping page...https://sandeepmj.github.io/scrape-example-page/heaviest-animals-page2.html\n",
      "snoozing for 12 seconds before scraping next link.\n",
      "got it...scraping page...https://sandeepmj.github.io/scrape-example-page/heaviest-animals-page3.html\n",
      "snoozing for 19 seconds before scraping next link.\n",
      "got it...scraping page...https://sandeepmj.github.io/scrape-example-page/heaviest-animals-page4.html\n",
      "snoozing for 14 seconds before scraping next link.\n",
      "oh no! https://sandeepmj.github.io/scrape-example-page/heaviest-animals-page5.html returned: 404\n"
     ]
    }
   ],
   "source": [
    "total_pages = 6\n",
    "for url_number in range(0,total_pages):\n",
    "    link = url.format(url_number)\n",
    "#     print(url_number)\n",
    "    site = requests.get(link)\n",
    "    try: \n",
    "        if site.status_code == 200:\n",
    "            print(f\"got it...scraping page...{link}\")\n",
    "            soup = BeautifulSoup(site.content, \"html.parser\")\n",
    "            snooze = randrange(10,25)\n",
    "            print(f\"snoozing for {snooze} seconds before scraping next link.\")\n",
    "            time.sleep(snooze)\n",
    "\n",
    "        else:\n",
    "            print(f\"oh no! {link} returned:\", site.status_code)\n",
    "    except: \n",
    "        print(f\"I can't seem to find these urls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Cleaning and Organizing Functions\n",
    "\n",
    "Before proceeding to the entire scrape, let's activate our functions that will help us clean and organize our scraped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# function to lowercase, strip and underscore header labels\n",
    "def santize_label(label):\n",
    "    value = label.lower().replace(\":\", \"\").strip()\n",
    "    value = re.sub(r'[^A-Za-z0-9]+', '_', value)\n",
    "    return value\n",
    "\n",
    "# function to create a dict of row data\n",
    "def make_dict_list(animal, weight, animal_type):\n",
    "    creature = {'animal': animal, 'weight': weight, 'animal_type': animal_type}\n",
    "    return creature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# All in One Step\n",
    "\n",
    "Because we are using a  ```for loop``` that cycles through each link to do multiple steps on our target data, we need to have it done as one step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "got it...scraping page...https://sandeepmj.github.io/scrape-example-page/heaviest-animals-page1.html\n",
      "[<td>Blue whale</td>, <td>136,000</td>, <td>Marine</td>]\n",
      "{'animal': 'Blue whale', 'weight': 136000, 'animal_type': 'Marine'}\n",
      "snoozing for 5 seconds before scraping next link.\n",
      "[<td>Bowhead whale</td>, <td>100,000</td>, <td>Marine</td>]\n",
      "{'animal': 'Bowhead whale', 'weight': 100000, 'animal_type': 'Marine'}\n",
      "snoozing for 4 seconds before scraping next link.\n",
      "[<td>Fin whale</td>, <td>70,000</td>, <td>Marine</td>]\n",
      "{'animal': 'Fin whale', 'weight': 70000, 'animal_type': 'Marine'}\n",
      "snoozing for 5 seconds before scraping next link.\n",
      "[<td>Southern right whale</td>, <td>45,000</td>, <td>Marine</td>]\n",
      "{'animal': 'Southern right whale', 'weight': 45000, 'animal_type': 'Marine'}\n",
      "snoozing for 3 seconds before scraping next link.\n",
      "[<td>Humpback whale</td>, <td>30,000</td>, <td>Marine</td>]\n",
      "{'animal': 'Humpback whale', 'weight': 30000, 'animal_type': 'Marine'}\n",
      "snoozing for 5 seconds before scraping next link.\n",
      "oh no! https://sandeepmj.github.io/scrape-example-page/heaviest-animals-page1.html returned: 200\n",
      "2\n",
      "got it...scraping page...https://sandeepmj.github.io/scrape-example-page/heaviest-animals-page2.html\n",
      "[<td>Gray whale</td>, <td>28,500</td>, <td>Marine</td>]\n",
      "{'animal': 'Gray whale', 'weight': 28500, 'animal_type': 'Marine'}\n",
      "snoozing for 3 seconds before scraping next link.\n",
      "[<td>Northern right whale</td>, <td>23,000</td>, <td>Marine</td>]\n",
      "{'animal': 'Northern right whale', 'weight': 23000, 'animal_type': 'Marine'}\n",
      "snoozing for 4 seconds before scraping next link.\n",
      "[<td>Sei whale</td>, <td>20,000</td>, <td>Marine</td>]\n",
      "{'animal': 'Sei whale', 'weight': 20000, 'animal_type': 'Marine'}\n",
      "snoozing for 4 seconds before scraping next link.\n",
      "[<td>Bryde's whale</td>, <td>16,000</td>, <td>Marine</td>]\n",
      "{'animal': \"Bryde's whale\", 'weight': 16000, 'animal_type': 'Marine'}\n",
      "snoozing for 5 seconds before scraping next link.\n",
      "[<td>Baird's beaked whale</td>, <td>11,380</td>, <td>Marine</td>]\n",
      "{'animal': \"Baird's beaked whale\", 'weight': 11380, 'animal_type': 'Marine'}\n",
      "snoozing for 3 seconds before scraping next link.\n",
      "oh no! https://sandeepmj.github.io/scrape-example-page/heaviest-animals-page2.html returned: 200\n",
      "3\n",
      "got it...scraping page...https://sandeepmj.github.io/scrape-example-page/heaviest-animals-page3.html\n",
      "[<td>Minke whale </td>, <td>7,500</td>, <td>Marine</td>]\n",
      "{'animal': 'Minke whale ', 'weight': 7500, 'animal_type': 'Marine'}\n",
      "snoozing for 3 seconds before scraping next link.\n",
      "[<td>Northern bottlenose whale</td>, <td>6,500</td>, <td>Marine</td>]\n",
      "{'animal': 'Northern bottlenose whale', 'weight': 6500, 'animal_type': 'Marine'}\n",
      "snoozing for 4 seconds before scraping next link.\n",
      "[<td>Gervais's beaked whale</td>, <td>5,600</td>, <td>Marine</td>]\n",
      "{'animal': \"Gervais's beaked whale\", 'weight': 5600, 'animal_type': 'Marine'}\n",
      "snoozing for 4 seconds before scraping next link.\n",
      "[<td>African elephant</td>, <td>4,800</td>, <td>Terrestrial</td>]\n",
      "{'animal': 'African elephant', 'weight': 4800, 'animal_type': 'Terrestrial'}\n",
      "snoozing for 5 seconds before scraping next link.\n",
      "[<td>Killer whale</td>, <td>3,988</td>, <td>Marine</td>]\n",
      "{'animal': 'Killer whale', 'weight': 3988, 'animal_type': 'Marine'}\n",
      "snoozing for 3 seconds before scraping next link.\n",
      "oh no! https://sandeepmj.github.io/scrape-example-page/heaviest-animals-page3.html returned: 200\n",
      "4\n",
      "got it...scraping page...https://sandeepmj.github.io/scrape-example-page/heaviest-animals-page4.html\n",
      "[<td>Hippopotamus</td>, <td>3,750</td>, <td>Terrestrial</td>]\n",
      "{'animal': 'Hippopotamus', 'weight': 3750, 'animal_type': 'Terrestrial'}\n",
      "[{'animal': 'Blue whale', 'weight': 136000, 'animal_type': 'Marine'}, {'animal': 'Bowhead whale', 'weight': 100000, 'animal_type': 'Marine'}, {'animal': 'Fin whale', 'weight': 70000, 'animal_type': 'Marine'}, {'animal': 'Southern right whale', 'weight': 45000, 'animal_type': 'Marine'}, {'animal': 'Humpback whale', 'weight': 30000, 'animal_type': 'Marine'}, {'animal': 'Gray whale', 'weight': 28500, 'animal_type': 'Marine'}, {'animal': 'Northern right whale', 'weight': 23000, 'animal_type': 'Marine'}, {'animal': 'Sei whale', 'weight': 20000, 'animal_type': 'Marine'}, {'animal': \"Bryde's whale\", 'weight': 16000, 'animal_type': 'Marine'}, {'animal': \"Baird's beaked whale\", 'weight': 11380, 'animal_type': 'Marine'}, {'animal': 'Minke whale ', 'weight': 7500, 'animal_type': 'Marine'}, {'animal': 'Northern bottlenose whale', 'weight': 6500, 'animal_type': 'Marine'}, {'animal': \"Gervais's beaked whale\", 'weight': 5600, 'animal_type': 'Marine'}, {'animal': 'African elephant', 'weight': 4800, 'animal_type': 'Terrestrial'}, {'animal': 'Killer whale', 'weight': 3988, 'animal_type': 'Marine'}, {'animal': 'Hippopotamus', 'weight': 3750, 'animal_type': 'Terrestrial'}]\n",
      "[<td>Asian elephant</td>, <td> 3,178</td>, <td>Terrestrial</td>]\n",
      "{'animal': 'Asian elephant', 'weight': 3178, 'animal_type': 'Terrestrial'}\n",
      "[{'animal': 'Blue whale', 'weight': 136000, 'animal_type': 'Marine'}, {'animal': 'Bowhead whale', 'weight': 100000, 'animal_type': 'Marine'}, {'animal': 'Fin whale', 'weight': 70000, 'animal_type': 'Marine'}, {'animal': 'Southern right whale', 'weight': 45000, 'animal_type': 'Marine'}, {'animal': 'Humpback whale', 'weight': 30000, 'animal_type': 'Marine'}, {'animal': 'Gray whale', 'weight': 28500, 'animal_type': 'Marine'}, {'animal': 'Northern right whale', 'weight': 23000, 'animal_type': 'Marine'}, {'animal': 'Sei whale', 'weight': 20000, 'animal_type': 'Marine'}, {'animal': \"Bryde's whale\", 'weight': 16000, 'animal_type': 'Marine'}, {'animal': \"Baird's beaked whale\", 'weight': 11380, 'animal_type': 'Marine'}, {'animal': 'Minke whale ', 'weight': 7500, 'animal_type': 'Marine'}, {'animal': 'Northern bottlenose whale', 'weight': 6500, 'animal_type': 'Marine'}, {'animal': \"Gervais's beaked whale\", 'weight': 5600, 'animal_type': 'Marine'}, {'animal': 'African elephant', 'weight': 4800, 'animal_type': 'Terrestrial'}, {'animal': 'Killer whale', 'weight': 3988, 'animal_type': 'Marine'}, {'animal': 'Hippopotamus', 'weight': 3750, 'animal_type': 'Terrestrial'}, {'animal': 'Asian elephant', 'weight': 3178, 'animal_type': 'Terrestrial'}]\n",
      "[<td>Cuvier's beaked whale</td>, <td>2,701</td>, <td>Marine</td>]\n",
      "{'animal': \"Cuvier's beaked whale\", 'weight': 2701, 'animal_type': 'Marine'}\n",
      "[{'animal': 'Blue whale', 'weight': 136000, 'animal_type': 'Marine'}, {'animal': 'Bowhead whale', 'weight': 100000, 'animal_type': 'Marine'}, {'animal': 'Fin whale', 'weight': 70000, 'animal_type': 'Marine'}, {'animal': 'Southern right whale', 'weight': 45000, 'animal_type': 'Marine'}, {'animal': 'Humpback whale', 'weight': 30000, 'animal_type': 'Marine'}, {'animal': 'Gray whale', 'weight': 28500, 'animal_type': 'Marine'}, {'animal': 'Northern right whale', 'weight': 23000, 'animal_type': 'Marine'}, {'animal': 'Sei whale', 'weight': 20000, 'animal_type': 'Marine'}, {'animal': \"Bryde's whale\", 'weight': 16000, 'animal_type': 'Marine'}, {'animal': \"Baird's beaked whale\", 'weight': 11380, 'animal_type': 'Marine'}, {'animal': 'Minke whale ', 'weight': 7500, 'animal_type': 'Marine'}, {'animal': 'Northern bottlenose whale', 'weight': 6500, 'animal_type': 'Marine'}, {'animal': \"Gervais's beaked whale\", 'weight': 5600, 'animal_type': 'Marine'}, {'animal': 'African elephant', 'weight': 4800, 'animal_type': 'Terrestrial'}, {'animal': 'Killer whale', 'weight': 3988, 'animal_type': 'Marine'}, {'animal': 'Hippopotamus', 'weight': 3750, 'animal_type': 'Terrestrial'}, {'animal': 'Asian elephant', 'weight': 3178, 'animal_type': 'Terrestrial'}, {'animal': \"Cuvier's beaked whale\", 'weight': 2701, 'animal_type': 'Marine'}]\n",
      "[<td>Short-finned pilot whale</td>, <td>2,200</td>, <td>Marine</td>]\n",
      "{'animal': 'Short-finned pilot whale', 'weight': 2200, 'animal_type': 'Marine'}\n",
      "[{'animal': 'Blue whale', 'weight': 136000, 'animal_type': 'Marine'}, {'animal': 'Bowhead whale', 'weight': 100000, 'animal_type': 'Marine'}, {'animal': 'Fin whale', 'weight': 70000, 'animal_type': 'Marine'}, {'animal': 'Southern right whale', 'weight': 45000, 'animal_type': 'Marine'}, {'animal': 'Humpback whale', 'weight': 30000, 'animal_type': 'Marine'}, {'animal': 'Gray whale', 'weight': 28500, 'animal_type': 'Marine'}, {'animal': 'Northern right whale', 'weight': 23000, 'animal_type': 'Marine'}, {'animal': 'Sei whale', 'weight': 20000, 'animal_type': 'Marine'}, {'animal': \"Bryde's whale\", 'weight': 16000, 'animal_type': 'Marine'}, {'animal': \"Baird's beaked whale\", 'weight': 11380, 'animal_type': 'Marine'}, {'animal': 'Minke whale ', 'weight': 7500, 'animal_type': 'Marine'}, {'animal': 'Northern bottlenose whale', 'weight': 6500, 'animal_type': 'Marine'}, {'animal': \"Gervais's beaked whale\", 'weight': 5600, 'animal_type': 'Marine'}, {'animal': 'African elephant', 'weight': 4800, 'animal_type': 'Terrestrial'}, {'animal': 'Killer whale', 'weight': 3988, 'animal_type': 'Marine'}, {'animal': 'Hippopotamus', 'weight': 3750, 'animal_type': 'Terrestrial'}, {'animal': 'Asian elephant', 'weight': 3178, 'animal_type': 'Terrestrial'}, {'animal': \"Cuvier's beaked whale\", 'weight': 2701, 'animal_type': 'Marine'}, {'animal': 'Short-finned pilot whale', 'weight': 2200, 'animal_type': 'Marine'}]\n",
      "[<td>White rhinoceros</td>, <td>2,175</td>, <td>Terrestrial</td>]\n",
      "{'animal': 'White rhinoceros', 'weight': 2175, 'animal_type': 'Terrestrial'}\n",
      "[{'animal': 'Blue whale', 'weight': 136000, 'animal_type': 'Marine'}, {'animal': 'Bowhead whale', 'weight': 100000, 'animal_type': 'Marine'}, {'animal': 'Fin whale', 'weight': 70000, 'animal_type': 'Marine'}, {'animal': 'Southern right whale', 'weight': 45000, 'animal_type': 'Marine'}, {'animal': 'Humpback whale', 'weight': 30000, 'animal_type': 'Marine'}, {'animal': 'Gray whale', 'weight': 28500, 'animal_type': 'Marine'}, {'animal': 'Northern right whale', 'weight': 23000, 'animal_type': 'Marine'}, {'animal': 'Sei whale', 'weight': 20000, 'animal_type': 'Marine'}, {'animal': \"Bryde's whale\", 'weight': 16000, 'animal_type': 'Marine'}, {'animal': \"Baird's beaked whale\", 'weight': 11380, 'animal_type': 'Marine'}, {'animal': 'Minke whale ', 'weight': 7500, 'animal_type': 'Marine'}, {'animal': 'Northern bottlenose whale', 'weight': 6500, 'animal_type': 'Marine'}, {'animal': \"Gervais's beaked whale\", 'weight': 5600, 'animal_type': 'Marine'}, {'animal': 'African elephant', 'weight': 4800, 'animal_type': 'Terrestrial'}, {'animal': 'Killer whale', 'weight': 3988, 'animal_type': 'Marine'}, {'animal': 'Hippopotamus', 'weight': 3750, 'animal_type': 'Terrestrial'}, {'animal': 'Asian elephant', 'weight': 3178, 'animal_type': 'Terrestrial'}, {'animal': \"Cuvier's beaked whale\", 'weight': 2701, 'animal_type': 'Marine'}, {'animal': 'Short-finned pilot whale', 'weight': 2200, 'animal_type': 'Marine'}, {'animal': 'White rhinoceros', 'weight': 2175, 'animal_type': 'Terrestrial'}]\n",
      "oh no! https://sandeepmj.github.io/scrape-example-page/heaviest-animals-page4.html returned: 200\n"
     ]
    }
   ],
   "source": [
    "total_pages = 5 ## number of pages we want to scrape\n",
    "\n",
    "data_rows = [] # list of dicts that hold row info\n",
    "\n",
    "for url_number in range(1,total_pages):\n",
    "    link = url.format(url_number)\n",
    "    print(url_number)\n",
    "#     print(f\"I'm on page {page}\")\n",
    "    site = requests.get(link)\n",
    "    try: \n",
    "        if site.status_code == 200:\n",
    "            print(f\"got it...scraping page...{link}\")\n",
    "            soup = BeautifulSoup(site.content, \"html.parser\")\n",
    "            \n",
    "                 # find table in soup\n",
    "        table = soup.find(\"table\", class_ =\"full_table\")\n",
    "    #     print(table.prettify())\n",
    "\n",
    "        # find rows\n",
    "        rows = table.find(\"tbody\").find_all(\"tr\")\n",
    "    #     print(rows)\n",
    "\n",
    "\n",
    "        ## grab each row into proper variable\n",
    "        for row in rows:\n",
    "            my_row = row.find_all(\"td\")\n",
    "            print(my_row)\n",
    "\n",
    "    #         lastname = my_row[0].getText().replace('\\n', \"\")\n",
    "            animal = my_row[0].get_text()\n",
    "            weight = int(my_row[1].get_text().replace(\",\",\"\"))\n",
    "            animal_type = my_row[2].get_text()\n",
    "\n",
    "    #         print(animal, weight, animal_type)\n",
    "    \n",
    "\n",
    "            ## CREATE DICT\n",
    "    #         creatures_dict = {'animal': animal, 'weight': weight, 'animal_type': animal_type}\n",
    "            \n",
    "            ## USE DICT CONSTRUCTOR\n",
    "            creatures_dict = dict(\n",
    "                animal = animal,\n",
    "                weight= weight,\n",
    "                animal_type= animal_type\n",
    "            )\n",
    "           \n",
    "            print(creatures_dict)\n",
    "\n",
    "            data_rows.append(creatures_dict)\n",
    "\n",
    "            if url_number != (total_pages-1):\n",
    "                snooze = randrange(3,6)\n",
    "                print(f\"snoozing for {snooze} seconds before scraping next link.\")\n",
    "                time.sleep(snooze)\n",
    "\n",
    "            else:\n",
    "                pass\n",
    "                print(data_rows)\n",
    "            \n",
    "        else:\n",
    "            print(f\"oh no! {link} returned:\", site.status_code)\n",
    "    except: \n",
    "        print(f\"I can't seem to find these urls\")\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'animal': 'Blue whale', 'weight': 136000, 'animal_type': 'Marine'},\n",
       " {'animal': 'Bowhead whale', 'weight': 100000, 'animal_type': 'Marine'},\n",
       " {'animal': 'Fin whale', 'weight': 70000, 'animal_type': 'Marine'},\n",
       " {'animal': 'Southern right whale', 'weight': 45000, 'animal_type': 'Marine'},\n",
       " {'animal': 'Humpback whale', 'weight': 30000, 'animal_type': 'Marine'},\n",
       " {'animal': 'Gray whale', 'weight': 28500, 'animal_type': 'Marine'},\n",
       " {'animal': 'Northern right whale', 'weight': 23000, 'animal_type': 'Marine'},\n",
       " {'animal': 'Sei whale', 'weight': 20000, 'animal_type': 'Marine'},\n",
       " {'animal': \"Bryde's whale\", 'weight': 16000, 'animal_type': 'Marine'},\n",
       " {'animal': \"Baird's beaked whale\", 'weight': 11380, 'animal_type': 'Marine'},\n",
       " {'animal': 'Minke whale ', 'weight': 7500, 'animal_type': 'Marine'},\n",
       " {'animal': 'Northern bottlenose whale',\n",
       "  'weight': 6500,\n",
       "  'animal_type': 'Marine'},\n",
       " {'animal': \"Gervais's beaked whale\", 'weight': 5600, 'animal_type': 'Marine'},\n",
       " {'animal': 'African elephant', 'weight': 4800, 'animal_type': 'Terrestrial'},\n",
       " {'animal': 'Killer whale', 'weight': 3988, 'animal_type': 'Marine'},\n",
       " {'animal': 'Hippopotamus', 'weight': 3750, 'animal_type': 'Terrestrial'},\n",
       " {'animal': 'Asian elephant', 'weight': 3178, 'animal_type': 'Terrestrial'},\n",
       " {'animal': \"Cuvier's beaked whale\", 'weight': 2701, 'animal_type': 'Marine'},\n",
       " {'animal': 'Short-finned pilot whale',\n",
       "  'weight': 2200,\n",
       "  'animal_type': 'Marine'},\n",
       " {'animal': 'White rhinoceros', 'weight': 2175, 'animal_type': 'Terrestrial'}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Export to a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## use pandas to write to csv file\n",
    "filename = \"heaviest-animals.csv\"\n",
    "# df = pd.DataFrame({key: pd.Series(value) for key, value in gas_dict.items()})\n",
    "df = pd.DataFrame(data_rows) \n",
    "df.to_csv(filename, encoding='utf-8', index=False)\n",
    "\n",
    "print(f\"{filename} is in your project folder!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Not needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "## not needed~!\n",
    "# find column headers\n",
    "header = table.find(\"thead\").find(\"tr\")\n",
    "print(header.prettify())\n",
    "labels = []\n",
    "for column_headers in header.find_all(\"th\", class_ =\"table-head\"):\n",
    "    column_header = santize_label(column_headers.get_text())\n",
    "    labels.append(column_header)\n",
    "labels.append('link')\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "all_urls_lc = [base_link.format(url_number) for url_number in range(1,total_pages)]\n",
    "all_urls_lc"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
