{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 710,
     "status": "ok",
     "timestamp": 1600347022898,
     "user": {
      "displayName": "Colleen Mcelroy",
      "photoUrl": "",
      "userId": "13977522149455724379"
     },
     "user_tz": 240
    },
    "id": "qhVSrceWHhCb"
   },
   "outputs": [],
   "source": [
    "## import libraries\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "# from google.colab import files  ## to export our files to our computer drive\n",
    "import random  ##iloc/loc\n",
    "\n",
    "## Let's import the required libaries to create a delay\n",
    "from random import randrange ##  allows us to randomize numbers library\n",
    "import time ## time tracker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single page scrape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to scrape a table that contains NFL player salaries for 2019. \n",
    "\n",
    "The webpage is ```https://sandeepmj.github.io/scrape-example-page/\"```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1492,
     "status": "ok",
     "timestamp": 1600347023693,
     "user": {
      "displayName": "Colleen Mcelroy",
      "photoUrl": "",
      "userId": "13977522149455724379"
     },
     "user_tz": 240
    },
    "id": "UD_4WFVaHhCf",
    "outputId": "e323393a-7920-4f49-b37e-fa3d622b73a3"
   },
   "outputs": [],
   "source": [
    "##scrape url website\n",
    "url = \"https://sandeepmj.github.io/scrape-example-page\"\n",
    "page = requests.get(url)\n",
    "print(page.status_code)  ## should print 200. checks http response code status\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1480,
     "status": "ok",
     "timestamp": 1600347023694,
     "user": {
      "displayName": "Colleen Mcelroy",
      "photoUrl": "",
      "userId": "13977522149455724379"
     },
     "user_tz": 240
    },
    "id": "Z8yo0W_MHhCi",
    "outputId": "421e945c-5ad9-43f3-8d9c-50c9df54cf36"
   },
   "outputs": [],
   "source": [
    "## turn into soup\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "print(type(soup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1467,
     "status": "ok",
     "timestamp": 1600347023695,
     "user": {
      "displayName": "Colleen Mcelroy",
      "photoUrl": "",
      "userId": "13977522149455724379"
     },
     "user_tz": 240
    },
    "id": "VeNoSuW9HhCl",
    "outputId": "7c024061-1b0a-403f-a1c0-c45e7c46e99e"
   },
   "outputs": [],
   "source": [
    "## MUST turn html into a string\n",
    "html = soup.prettify()\n",
    "print(type(html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1457,
     "status": "ok",
     "timestamp": 1600347023698,
     "user": {
      "displayName": "Colleen Mcelroy",
      "photoUrl": "",
      "userId": "13977522149455724379"
     },
     "user_tz": 240
    },
    "id": "MeNtL-t7HhCn",
    "outputId": "ee380d2f-8200-4cbb-90ee-fe066bef763f"
   },
   "outputs": [],
   "source": [
    "## use Pandas to read tables on page\n",
    "df = pd.read_html(html)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1583,
     "status": "ok",
     "timestamp": 1600347023841,
     "user": {
      "displayName": "Colleen Mcelroy",
      "photoUrl": "",
      "userId": "13977522149455724379"
     },
     "user_tz": 240
    },
    "id": "wuJ4Ti0GHhCq",
    "outputId": "ed553f3d-075e-4f1b-d3c0-34ca6b142702"
   },
   "outputs": [],
   "source": [
    "## Do we want the first table?\n",
    "df[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Do we want the the second table?\n",
    "df[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1576,
     "status": "ok",
     "timestamp": 1600347023842,
     "user": {
      "displayName": "Colleen Mcelroy",
      "photoUrl": "",
      "userId": "13977522149455724379"
     },
     "user_tz": 240
    },
    "id": "gBL1Kfx3HhCs"
   },
   "outputs": [],
   "source": [
    "## store it into a copy called nfl_df\n",
    "fl_df = df[1].copy() \n",
    "fl_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple page scrape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to scrape the tables scattered across this site\n",
    "\n",
    "https://sandeepmj.github.io/scrape-example-page/heaviest-animals-page1.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## How is it different?\n",
    "url = \"https://sandeepmj.github.io/scrape-example-page/heaviest-animals-page{}.html\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### We know we need a placeholder value of upto ```4```\n",
    "\n",
    "### Let's create a variable called  ```total_pages``` to match number of pages on site.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## total pages to scrape\n",
    "total_pages = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_pages = 5 ## number of pages we want to scrape\n",
    "df_all = [] ## list that will hold all the dataframes that are produced\n",
    "for url_number in range(1,total_pages):\n",
    "    link = url.format(url_number)\n",
    "    print(url_number)\n",
    "#     print(f\"I'm on page {page}\")\n",
    "    site = requests.get(link)\n",
    "    try: \n",
    "        if site.status_code == 200:\n",
    "            print(f\"got it...scraping page...{link}\")\n",
    "            soup = BeautifulSoup(site.content, \"html.parser\")\n",
    "            html = soup.prettify()  ## turn soup into html \n",
    "            df = pd.read_html(html) ## turn html table into a df using pandas\n",
    "            df_all.append(df[0]) ## append that first table to a list\n",
    "            ## let's not forget to snooze\n",
    "            snooze = randrange(5,7)\n",
    "            print(f\"snoozing for {snooze} seconds before scraping next link.\")\n",
    "            time.sleep(snooze)\n",
    "\n",
    "        else:\n",
    "            print(f\"oh no! {link} returned:\", site.status_code)\n",
    "    except: \n",
    "        print(f\"I can't seem to find these urls\")\n",
    "        \n",
    "df_all ## what does our list look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FUNCTION to download individual dataframes in a list as a single csv\n",
    "def combine_tables(list_name,filename):\n",
    "  '''\n",
    "  Takes dataframes in a l ist and combines into a single CSV.\n",
    "  Tables must have identical column headers and order\n",
    "  Arguments: name of list produced by tabula and the CSV name you want (in quotes as a string)\n",
    "  '''\n",
    "  dataframes = [pd.DataFrame(a_table) for a_table in list_name] ## list comprehension to turn each tabula table into a dataframe\n",
    "  df = pd.concat(dataframes) ## join/concat all the dataframes into one dataframe\n",
    "  df.reset_index(inplace = True, drop = True) ## reset index, drop what was there before\n",
    "  df.to_csv(filename, encoding='utf-8', index=False) ## convert that single dataframe into a csv\n",
    "#   files.download(filename) ## download it\n",
    "  print(f\"{filename} is in your downloads folder!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## call that function\n",
    "combine_tables(df_all,\"animals_today.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "RVIA.Sandeep.ipynb",
   "provenance": [
    {
     "file_id": "19IrKyzkGCq8R2X_RCH5RG4Rhe8xr3668",
     "timestamp": 1600346225593
    },
    {
     "file_id": "1D75QB2XYRMsyUpcKm63ee0GDycfX-g-i",
     "timestamp": 1599872538239
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
