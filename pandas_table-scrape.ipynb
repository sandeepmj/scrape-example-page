{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 710,
     "status": "ok",
     "timestamp": 1600347022898,
     "user": {
      "displayName": "Colleen Mcelroy",
      "photoUrl": "",
      "userId": "13977522149455724379"
     },
     "user_tz": 240
    },
    "id": "qhVSrceWHhCb"
   },
   "outputs": [],
   "source": [
    "## import libraries\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "# from google.colab import files  ## to export our files to our computer drive\n",
    "import random  ##iloc/loc\n",
    "\n",
    "## Let's import the required libaries to create a delay\n",
    "from random import randrange ##  allows us to randomize numbers library\n",
    "import time ## time tracker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single page scrape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to scrape a table that contains NFL player salaries for 2019. \n",
    "\n",
    "The webpage is ```https://sandeepmj.github.io/scrape-example-page/\"```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1492,
     "status": "ok",
     "timestamp": 1600347023693,
     "user": {
      "displayName": "Colleen Mcelroy",
      "photoUrl": "",
      "userId": "13977522149455724379"
     },
     "user_tz": 240
    },
    "id": "UD_4WFVaHhCf",
    "outputId": "e323393a-7920-4f49-b37e-fa3d622b73a3"
   },
   "outputs": [],
   "source": [
    "##scrape url website\n",
    "url = \"https://sandeepmj.github.io/scrape-example-page\"\n",
    "page = requests.get(url)\n",
    "print(page.status_code)  ## should print 200. checks http response code status\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1480,
     "status": "ok",
     "timestamp": 1600347023694,
     "user": {
      "displayName": "Colleen Mcelroy",
      "photoUrl": "",
      "userId": "13977522149455724379"
     },
     "user_tz": 240
    },
    "id": "Z8yo0W_MHhCi",
    "outputId": "421e945c-5ad9-43f3-8d9c-50c9df54cf36"
   },
   "outputs": [],
   "source": [
    "## turn into soup\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "print(type(soup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1467,
     "status": "ok",
     "timestamp": 1600347023695,
     "user": {
      "displayName": "Colleen Mcelroy",
      "photoUrl": "",
      "userId": "13977522149455724379"
     },
     "user_tz": 240
    },
    "id": "VeNoSuW9HhCl",
    "outputId": "7c024061-1b0a-403f-a1c0-c45e7c46e99e"
   },
   "outputs": [],
   "source": [
    "## MUST turn html into a string\n",
    "html = soup.prettify()\n",
    "print(type(html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1457,
     "status": "ok",
     "timestamp": 1600347023698,
     "user": {
      "displayName": "Colleen Mcelroy",
      "photoUrl": "",
      "userId": "13977522149455724379"
     },
     "user_tz": 240
    },
    "id": "MeNtL-t7HhCn",
    "outputId": "ee380d2f-8200-4cbb-90ee-fe066bef763f"
   },
   "outputs": [],
   "source": [
    "## use Pandas to read tables on page\n",
    "df = pd.read_html(html)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1583,
     "status": "ok",
     "timestamp": 1600347023841,
     "user": {
      "displayName": "Colleen Mcelroy",
      "photoUrl": "",
      "userId": "13977522149455724379"
     },
     "user_tz": 240
    },
    "id": "wuJ4Ti0GHhCq",
    "outputId": "ed553f3d-075e-4f1b-d3c0-34ca6b142702"
   },
   "outputs": [],
   "source": [
    "## Do we want the first table?\n",
    "df[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Do we want the the second table?\n",
    "df[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1576,
     "status": "ok",
     "timestamp": 1600347023842,
     "user": {
      "displayName": "Colleen Mcelroy",
      "photoUrl": "",
      "userId": "13977522149455724379"
     },
     "user_tz": 240
    },
    "id": "gBL1Kfx3HhCs"
   },
   "outputs": [],
   "source": [
    "## store it into a copy called nfl_df\n",
    "fl_df = df[1].copy() \n",
    "fl_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple page scrape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to scrape the tables scattered across this site\n",
    "\n",
    "https://sandeepmj.github.io/scrape-example-page/heaviest-animals-page1.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## How is it different?\n",
    "url = \"https://sandeepmj.github.io/scrape-example-page/heaviest-animals-page{}.html\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### We know we need a placeholder value of upto ```4```\n",
    "\n",
    "### Let's create a variable called  ```total_pages``` to match number of pages on site.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## total pages to scrape\n",
    "total_pages = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://sandeepmj.github.io/scrape-example-page/heaviest-animals-page0.html\n",
      "404\n",
      "https://sandeepmj.github.io/scrape-example-page/heaviest-animals-page1.html\n",
      "200\n",
      "https://sandeepmj.github.io/scrape-example-page/heaviest-animals-page2.html\n",
      "200\n",
      "https://sandeepmj.github.io/scrape-example-page/heaviest-animals-page3.html\n",
      "200\n",
      "https://sandeepmj.github.io/scrape-example-page/heaviest-animals-page4.html\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "## Let's write the for loop\n",
    "## but instead of storing into a list, we just feed it directly to our placeholder\n",
    "## we want to just scape each page\n",
    "for url_number in range(0,total_pages):\n",
    "    link = url.format(url_number)\n",
    "    print(link)\n",
    "    site = requests.get(link)\n",
    "    print(site.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "got it...scraping page...https://sandeepmj.github.io/scrape-example-page/heaviest-animals-page1.html\n",
      "<class 'str'>\n",
      "snoozing for 5 seconds before scraping next link.\n",
      "I can't seem to find these urls\n",
      "2\n",
      "got it...scraping page...https://sandeepmj.github.io/scrape-example-page/heaviest-animals-page2.html\n",
      "<class 'str'>\n",
      "snoozing for 5 seconds before scraping next link.\n",
      "I can't seem to find these urls\n",
      "3\n",
      "got it...scraping page...https://sandeepmj.github.io/scrape-example-page/heaviest-animals-page3.html\n",
      "<class 'str'>\n",
      "snoozing for 5 seconds before scraping next link.\n",
      "I can't seem to find these urls\n",
      "4\n",
      "got it...scraping page...https://sandeepmj.github.io/scrape-example-page/heaviest-animals-page4.html\n",
      "<class 'str'>\n",
      "snoozing for 6 seconds before scraping next link.\n",
      "I can't seem to find these urls\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[                 Animal  Weight in KGs    Type\n",
       " 0            Blue whale         136000  Marine\n",
       " 1         Bowhead whale         100000  Marine\n",
       " 2             Fin whale          70000  Marine\n",
       " 3  Southern right whale          45000  Marine\n",
       " 4        Humpback whale          30000  Marine,\n",
       "                  Animal  Weight in kilograms    Type\n",
       " 0            Gray whale                28500  Marine\n",
       " 1  Northern right whale                23000  Marine\n",
       " 2             Sei whale                20000  Marine\n",
       " 3         Bryde's whale                16000  Marine\n",
       " 4  Baird's beaked whale                11380  Marine,\n",
       "                       Animal  Weight (KG)         Type\n",
       " 0                Minke whale         7500       Marine\n",
       " 1  Northern bottlenose whale         6500       Marine\n",
       " 2     Gervais's beaked whale         5600       Marine\n",
       " 3           African elephant         4800  Terrestrial\n",
       " 4               Killer whale         3988       Marine,\n",
       "                      Animal  Weight in kilograms         Type\n",
       " 0              Hippopotamus                 3750  Terrestrial\n",
       " 1            Asian elephant                 3178  Terrestrial\n",
       " 2     Cuvier's beaked whale                 2701       Marine\n",
       " 3  Short-finned pilot whale                 2200       Marine\n",
       " 4          White rhinoceros                 2175  Terrestrial]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_pages = 5 ## number of pages we want to scrape\n",
    "df_all = []\n",
    "for url_number in range(1,total_pages):\n",
    "    link = url.format(url_number)\n",
    "    print(url_number)\n",
    "#     print(f\"I'm on page {page}\")\n",
    "    site = requests.get(link)\n",
    "    try: \n",
    "        if site.status_code == 200:\n",
    "            print(f\"got it...scraping page...{link}\")\n",
    "            soup = BeautifulSoup(site.content, \"html.parser\")\n",
    "            ## turn soup into html \n",
    "            html = soup.prettify()\n",
    "            print(type(html))\n",
    "            df = pd.read_html(html)\n",
    "            df_all.append(df[0])\n",
    "\n",
    "            \n",
    "            \n",
    "            snooze = randrange(5,7)\n",
    "            print(f\"snoozing for {snooze} seconds before scraping next link.\")\n",
    "            time.sleep(snooze)\n",
    "            counter+=1\n",
    "\n",
    "        else:\n",
    "            print(f\"oh no! {link} returned:\", site.status_code)\n",
    "    except: \n",
    "        print(f\"I can't seem to find these urls\")\n",
    "        \n",
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## write function to combine tabula tables into a single csv\n",
    "## write function to combine tabula tables into a single csv\n",
    "def combine_tables(list_name,filename):\n",
    "  '''\n",
    "  Takes tables produced by tabula and combines into a single CSV.\n",
    "  Arguments: name of list produced by tabula and the CSV name you want (in quotes as a string)\n",
    "  '''\n",
    "  dataframes = [pd.DataFrame(a_table) for a_table in list_name] ## list comprehension to turn each tabula table into a dataframe\n",
    "  df = pd.concat(dataframes) ## join/concat all the dataframes into one dataframe\n",
    "  df.reset_index(inplace = True, drop = True) ## reset index, drop what was there before\n",
    "  df.to_csv(filename, encoding='utf-8', index=False) ## convert that single dataframe into a csv\n",
    "#   files.download(filename) ## download it\n",
    "  print(f\"{filename} is in your downloads folder!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "animals_today.csv is in your downloads folder!\n"
     ]
    }
   ],
   "source": [
    "combine_tables(df_all,\"animals_today.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "RVIA.Sandeep.ipynb",
   "provenance": [
    {
     "file_id": "19IrKyzkGCq8R2X_RCH5RG4Rhe8xr3668",
     "timestamp": 1600346225593
    },
    {
     "file_id": "1D75QB2XYRMsyUpcKm63ee0GDycfX-g-i",
     "timestamp": 1599872538239
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
